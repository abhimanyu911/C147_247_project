The CNN + GRU Hybrid Model processes the sEMG input to provide a prediction of keystrokes. 

The implemented CNN architecture processes a tensor input with a shape which corresponds to (batch_size, sequence_length, input_dimensions). The input is first projected through a linear layer with a set hidden dimension size before being passed through the 1D convolution layer. Afterwards, it is followed by a ReLU activation function and layer normalization. The primary function of the CNN within this architecture is to extract the features from the sequential sEMG data.
	
A Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN) that is designed to handle sequential data and long-term dependencies. This is one type of gated RNN used to control the flow of information in making predictions. Since we are working with a sequence of signals, the GRU is designed to capture the temporal relationship. While CNNâ€™s are able to process local dependencies well, however, struggle with long-term dependencies. Thus, two GRU layers were added, with each layer processing the feature sequences extracted with the CNN in both the forward and backward directions, enabling the model to capture contextual information from both past and future timesteps. This bidirectional setup allows the model to fully utilize the temporal relationships in the input sequence, improving its ability to understand complex patterns that span across the entire sequence. The GRU output is then passed through the final linear layer to integrate and transform the learned features into a final prediction. This architecture ensures that both local feature extraction from the CNN and long-range temporal dependencies from the GRU contribute to the model's output.

Specifically, with the CNN-GRU Hybrid architecture, the bidirectional GRU processes the feature sequences extracted by the preceding CNN layers. The GRU architecture is particularly useful because it is designed to capture temporal relationship
